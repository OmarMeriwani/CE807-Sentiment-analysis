{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFIDF Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMeriwani/CE807-Sentiment-analysis/blob/master/TFIDF_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "BOdJ_nDhby0N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This document shows the final classifier that has been used in this project which is based on TFIDF only."
      ]
    },
    {
      "metadata": {
        "id": "ksEaTf5gbnmM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "pd.options.mode.chained_assignment = None\n",
        "import numpy as np \n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tqdm import tqdm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import os\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "JavaOn = False\n",
        "\n",
        "if JavaOn == True:\n",
        "    java_path = \"C:/Program Files/Java/jdk1.8.0_161/bin/java.exe\"\n",
        "    os.environ['JAVAHOME'] = java_path\n",
        "    host='http://localhost'\n",
        "    port=9000\n",
        "    scnlp =StanfordCoreNLP(host, port=port,lang='en', timeout=30000)\n",
        "stemmer = PorterStemmer()\n",
        "print ('start')\n",
        "\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer \n",
        "tokenizer = TweetTokenizer()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oy9p4981bzgl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Previously explained methods for loading files, and preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "6IjpeD3vbznU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "vocab_filename = 'vocabulary.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "vocab = [v.lower() for v in vocab]\n",
        "\n",
        "\n",
        "def doc_to_clean_lines(doc, vocab):\n",
        "    clean_lines = ''\n",
        "    lines = doc.splitlines()\n",
        "\n",
        "    for line in lines:\n",
        "        tokens = line.split()\n",
        "        table = str.maketrans('', '', punctuation)\n",
        "        tokens = [w.translate(table) for w in tokens]\n",
        "        clean_lines = ' '.join(tokens)\n",
        "    return clean_lines\n",
        "\n",
        "\n",
        "def readfile(filename, istraining):\n",
        "    df = pd.read_csv(filename,header=0,sep='\\t')\n",
        "    mode = 'all' #all sentences or only full reviews (sentence,full)\n",
        "    data = []\n",
        "    prev = ''\n",
        "    for i in range(0,len(df)):\n",
        "        if mode == 'sentence':\n",
        "            if prev != str(df.loc[i][1]):\n",
        "                sentence = df.loc[i][2]\n",
        "                prev = str(df.loc[i][1])\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            sentence = df.loc[i][2]\n",
        "        if istraining == True:\n",
        "            reviewPolarity = int(df.loc[i][3])\n",
        "        sentence = doc_to_clean_lines(sentence,vocab)\n",
        "        if istraining == True:\n",
        "            data.append([sentence,reviewPolarity])\n",
        "        else:\n",
        "            data.append(sentence)\n",
        "    return data\n",
        "\n",
        "def split(docs, percentage):\n",
        "    length = len(docs)\n",
        "    firstlength = int (length * percentage)\n",
        "    training = docs[:firstlength]\n",
        "    test = docs[firstlength:length]\n",
        "    return training,test\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "arBj4fC_bztZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For final usage of sentences we used either POS tags or sentences with different choices regarding stop words, lemmatization and stemming."
      ]
    },
    {
      "metadata": {
        "id": "YlZgWlVbcfdT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess(docs):\n",
        "    result = []\n",
        "    for i in docs:\n",
        "        tokens = tokenizer.tokenize(i)\n",
        "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "        tokens = [stemmer.stem(str(t).lower()) for t in tokens]\n",
        "        # tokens = [t for t in tokens if t not in stop_words]\n",
        "        result.append(' '.join(tokens))\n",
        "    result = list(result)\n",
        "    return result\n",
        "def postags(docs):\n",
        "    result = []\n",
        "    if JavaOn == True:\n",
        "        for i in docs:\n",
        "            postags_list = []\n",
        "            for word,postag in scnlp.pos_tag(i):\n",
        "                postags_list.append(postag)\n",
        "            result.append(' '.join(postags_list))\n",
        "        result = list(result)\n",
        "    return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KMi2AYEqcfvm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reading data and separating datasets, the code shows the final implementation where the percentage is set to 99% of training data, but before it was set to smaller values to do inner test."
      ]
    },
    {
      "metadata": {
        "id": "6cpFBh2zcvGI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = np.array(readfile('train.csv',True))\n",
        "print ('Reading final test dataset')\n",
        "data2 = np.array(readfile('test.csv',False))\n",
        "\n",
        "print(data.shape)\n",
        "#print(data[:,0])\n",
        "traindata, testdata = split(data,0.99)\n",
        "print(testdata.shape)\n",
        "print(traindata.shape)\n",
        "train_docs = traindata[:,0]\n",
        "test_docs = testdata[:,0]\n",
        "y_train = traindata[:,1]\n",
        "y_test = testdata[:,1]\n",
        "x_train = preprocess(train_docs)\n",
        "x_test = preprocess(test_docs)\n",
        "print ('Preprocessing for final test dataset')\n",
        "x_test_final = preprocess(data2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZdUE3W64cvNA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fitting and transforming sentences into TFIDF vectors"
      ]
    },
    {
      "metadata": {
        "id": "WGHPySn7dF_c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(analyzer='word')\n",
        "_ = tfidf.fit(x_train)\n",
        "train_tfidf = tfidf.transform(x_train)\n",
        "test_tfidf = tfidf.transform(x_test)\n",
        "print ('Transforming final test dataset')\n",
        "test_final_tfidf = tfidf.transform(x_test_final)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h7YMwOd8dGIC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using LinearSVC as a machine learning model and storing the model separatly. Then storing the result of test dataset."
      ]
    },
    {
      "metadata": {
        "id": "5LhlUFMbbzzV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "model1 = LinearSVC()\n",
        "model1.fit(train_tfidf,y_train)\n",
        "print(model1.score(test_tfidf,y_test))\n",
        "y_pred = model1.predict(test_final_tfidf)\n",
        "print (y_pred)\n",
        "import pickle\n",
        "#pickle.dump(model1,open('LinearSVC.mdl','w'))\n",
        "\n",
        "df = pd.read_csv('test.csv', header=0, sep='\\t')\n",
        "new_df = pd.DataFrame(columns=['PhraseId','SentenceId',\t'Phrase', 'Sentiment'])\n",
        "counter = 0\n",
        "for i in range(0, len(df)):\n",
        "    new_df.loc[counter] = [df.loc[i][0],df.loc[i][1],df.loc[i][2],y_pred[counter]]\n",
        "    counter += 1\n",
        "new_df.to_csv('test_new.csv',sep='\\t')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}