{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMeriwani/CE807-Sentiment-analysis/blob/master/Word2Vec_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZxDa-YBFWfgD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This document provides the code of Word2vec classifer"
      ]
    },
    {
      "metadata": {
        "id": "8XXCRXypWc0g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "import pandas as pd\n",
        "from numpy import zeros\n",
        "from numpy import asarray\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bvjZel8aWgX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load files and clean sentences, remove punctuation, lowerize case and return clean setnences."
      ]
    },
    {
      "metadata": {
        "id": "I8sfeFV3WgeS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "    file = open(filename, 'r')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "def doc_to_clean_lines(doc, vocab):\n",
        "    clean_lines = ''\n",
        "    lines = doc.splitlines()\n",
        "\n",
        "    for line in lines:\n",
        "        tokens = line.split()\n",
        "        table = str.maketrans('', '', punctuation)\n",
        "        tokens = [w.translate(table) for w in tokens]\n",
        "        tokens = [w for w in tokens if w.lower() in vocab]\n",
        "        clean_lines = ' '.join(tokens)\n",
        "    return clean_lines\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hmPALRMQWglh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load [pre-created](https://github.com/OmarMeriwani/CE807-Sentiment-analysis/blob/master/Build_Word2Vec_Vocabulary.ipynb) vocabulary file"
      ]
    },
    {
      "metadata": {
        "id": "tH3k6HT0WgsY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_filename = 'vocabulary.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = vocab.split()\n",
        "vocab = set(vocab)\n",
        "vocab = [v.lower() for v in vocab]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sE1naCunWgzc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A method to read training dataset, getting values from it, and cleaning the sentences "
      ]
    },
    {
      "metadata": {
        "id": "V5lxIcmVWg5Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def readfile(filename):\n",
        "    df = pd.read_csv(filename,header=0,sep='\\t')\n",
        "    mode = 'sentence' #all sentences or only full reviews (sentence,full)\n",
        "    data = []\n",
        "    prev = ''\n",
        "    for i in range(0,len(df)):\n",
        "        if mode == 'sentence':\n",
        "            if prev != str(df.loc[i][1]):\n",
        "                sentence = df.loc[i][2]\n",
        "                prev = str(df.loc[i][1])\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            sentence = df.loc[i][2]\n",
        "        reviewPolarity = int(df.loc[i][3])\n",
        "        sentence = doc_to_clean_lines(sentence,vocab)\n",
        "        data.append([sentence,reviewPolarity])\n",
        "    return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5tD1dltXZFza",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To load  non binary files of embeddings"
      ]
    },
    {
      "metadata": {
        "id": "ApUmuOGHZHZT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_embedding(filename):\n",
        "\tfile = open(filename,'r')\n",
        "\tlines = file.readlines()\n",
        "\tfile.close()\n",
        "\tembedding = dict()\n",
        "\tfor line in lines:\n",
        "\t\tparts = line.split()\n",
        "\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
        "\treturn embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mKdrMWIWZbX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A method to get vectors for each word "
      ]
    },
    {
      "metadata": {
        "id": "sWxZthfsZbfr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_weight_matrix2(embedding, vocab):\n",
        "\tvocab_size = len(vocab) + 1\n",
        "\tweight_matrix = zeros((vocab_size, 300))\n",
        "\tfor word, i in vocab:\n",
        "\t\tvector = None\n",
        "\t\ttry:\n",
        "\t\t\tvector = embedding.get_vector(word)\n",
        "\t\texcept:\n",
        "\t\t\tcontinue\n",
        "\t\tif vector is not None:\n",
        "\t\t\tweight_matrix[i] = vector\n",
        "\treturn weight_matrix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xublGKblWhAC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Split the dataset into training and testing datasets according a percentage"
      ]
    },
    {
      "metadata": {
        "id": "cdJ_YABjWhHG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split(docs, percentage):\n",
        "    length = len(docs)\n",
        "    firstlength = int (length * percentage)\n",
        "    training = docs[:firstlength]\n",
        "    test = docs[firstlength:length]\n",
        "    return training,test\n",
        "data = np.array(readfile('train.csv'))\n",
        "print(data.shape)\n",
        "traindata, testdata = split(data,0.7)\n",
        "print(testdata.shape)\n",
        "print(traindata.shape)\n",
        "train_docs = traindata[:,0]\n",
        "test_docs = testdata[:,0]\n",
        "y_train = traindata[:,1]\n",
        "y_test = testdata[:,1]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "esX-k5BrWhOK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training word2vec model using the training tokens that have been created above"
      ]
    },
    {
      "metadata": {
        "id": "4wNGOUgRWhUg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w2v = Word2Vec(size=100, min_count=10)\n",
        "w2v.build_vocab(sentences=training_tokens)\n",
        "w2v.train(sentences=training_tokens,total_words=len(vocab),epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UcGAVU2_YX-6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Encode the documents using keras tokenizer, which will help in preioritizing words according to their occurences in the file."
      ]
    },
    {
      "metadata": {
        "id": "MhH8Z9a6YYJB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train_docs)\n",
        "# sequence encode\n",
        "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l3lGHrfHYYQJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Ensuring that all sequences in the resulting encoded array have the same length. Then, applying the same previous step on test data."
      ]
    },
    {
      "metadata": {
        "id": "cIaK4sKUYYWZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# pad sequences\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "# define training labels\n",
        "ytrain = traindata[:,1]\n",
        "test_docs = testdata[:,0]\n",
        "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "ytest = testdata[:,1]\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5bn_3IRvabsC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The commented code below shows two versions of embeddings, the ready one that is based on Google news and the one that has been created in this project."
      ]
    },
    {
      "metadata": {
        "id": "oeH2kdsDab0x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#raw_embedding = load_embedding('embedding2.txt')\n",
        "wv_from_bin = KeyedVectors.load_word2vec_format(datapath('G:/Data/GN/GoogleNews-vectors-negative300.bin'), binary=True)\n",
        "#embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
        "embedding_vectors = get_weight_matrix2(wv_from_bin, tokenizer.word_index.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "72c9NNRza3vd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Keras model for predicting the result (commented code shows the use of Logistic regression which has given similar results)"
      ]
    },
    {
      "metadata": {
        "id": "ixF9mwtTa36t",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embedding_layer = Embedding(vocab_size, 300, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(Dense(128, activation='relu', input_dim=200))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "\n",
        "# fit network\n",
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
        "#model = LogisticRegression(C=0.2, dual=True)\n",
        "#model.fit(Xtrain, ytrain)\n",
        "\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "#result = model.score(Xtest,ytest)\n",
        "print('Test Accuracy: %f' % (acc*100))\n",
        "#print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}