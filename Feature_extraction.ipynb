{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature extraction.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMeriwani/CE807-Sentiment-analysis/blob/master/Feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "AOQaO6qXDdG5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The libraries that have been used in feature extraction, a mixture of tools between nltk tools and Stanford tools using Java local API."
      ]
    },
    {
      "metadata": {
        "id": "IpmrACBEDZaI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import os\n",
        "from senticnet.senticnet import SenticNet\n",
        "from nltk.stem.porter import *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "elx1ReAFDbH0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Java parameters are needed to be set before starting, and to declare the class that is going to be used for stanford services."
      ]
    },
    {
      "metadata": {
        "id": "g-j4rJtCDbw1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "java_path = \"C:/Program Files/Java/jdk1.8.0_161/bin/java.exe\"\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "host='http://localhost'\n",
        "port=9000\n",
        "scnlp =StanfordCoreNLP(host, port=port,lang='en', timeout=30000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NqHTD7H7Egwi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Porter stemmer is used in the project generally"
      ]
    },
    {
      "metadata": {
        "id": "Io6_BSqsDb6l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gB6DJ1rfEgNB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As one of the methods that has been used to create polarity for sentences, we used the method that has been discussed in litrature review, where we get the maximum value and the average value of the words that have polarity in Senticnet tool. In case of negation words, we give the same polarity value multiplied by -1."
      ]
    },
    {
      "metadata": {
        "id": "7TlIufutDcFE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def SentimentsPolarity(sentence):\n",
        "    sn = SenticNet()\n",
        "    values = []\n",
        "    maxPolarity = 0.0\n",
        "    prev = ''\n",
        "    for stem in sentence:\n",
        "        s = stemmer.stem(stem)\n",
        "        try:\n",
        "            polarity_value = sn.polarity_intense(s)\n",
        "            pv = float(polarity_value)\n",
        "            #In case of negation, we give the opposite polarity value to the word.\n",
        "            if prev in ['not','no','never']:\n",
        "                pv = pv * -1\n",
        "            absPolarityVal = abs(pv)\n",
        "            if absPolarityVal > abs(float(maxPolarity)):\n",
        "                maxPolarity = pv\n",
        "            values.append(pv)\n",
        "        except Exception as e:\n",
        "            prev = s\n",
        "            continue\n",
        "        prev = s\n",
        "    if len(values) == 0:\n",
        "        return 0.0,maxPolarity\n",
        "    avg = float(sum(values) / len(values)).__round__(3)\n",
        "    return avg,maxPolarity\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e0H-3jQxI3o4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For the details of the below CSV files that are used in feature extraction, please refer to the file [ngrams_Polarity_Value.ipynb](https://github.com/OmarMeriwani/CE807-Sentiment-analysis/blob/master/ngrams_Polarity_Value.ipynb)."
      ]
    },
    {
      "metadata": {
        "id": "f-qHBv-pDcJE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_pbigrams = pd.read_csv('BigramsPolarity.csv',header=0,sep=',')\n",
        "pbigrams = df_pbigrams.values.tolist()\n",
        "\n",
        "df_punigrams = pd.read_csv('UnigramsPolarity.csv',header=0,sep=',')\n",
        "punigrams = df_punigrams.values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_KHdRGG3JIyz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To understand the extraction of the following lists please refer to [the file](https://github.com/OmarMeriwani/CE807-Sentiment-analysis/blob/master/Frequent_Subjective_Words.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "H8oc9fMsDcMM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_positiveWords = pd.read_csv('PositiveWords.csv',header=0,sep=',')\n",
        "positiveWords = df_positiveWords.values.tolist()\n",
        "\n",
        "df_negativeWords = pd.read_csv('NegativeWords.csv',header=0,sep=',')\n",
        "negativeWords = df_negativeWords.values.tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OWjTXxUnMq_l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Three ngrams of POS sequences were used, 3grams, 4grams and 5grams, to understand the extraction of these features please refer to the file of [POS nGrams](https://github.com/OmarMeriwani/CE807-Sentiment-analysis/blob/master/POS_Ngrams.ipynb).\n"
      ]
    },
    {
      "metadata": {
        "id": "W_i4RhhlEeCI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_posseq = pd.read_csv('POSTrigrams.csv',header=None,sep=',')\n",
        "tgrams = df_posseq.values.tolist()\n",
        "\n",
        "df_posseq = pd.read_csv('POSQgrams.csv',header=None,sep=',')\n",
        "qgrams = df_posseq.values.tolist()\n",
        "\n",
        "df_posseq = pd.read_csv('POSPgrams.csv',header=None,sep=',')\n",
        "pgrams = df_posseq.values.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dhl152TIM10s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The negation is separated from stop words to execlude the negation effect on subjective terms"
      ]
    },
    {
      "metadata": {
        "id": "ChL3i6CRM-CY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "stop_words = [s for s in stop_words if s not in ['no', 'not', 'never', 'nâ€™t', 'nt']]\n",
        "df = pd.read_csv('train.csv',header=0,sep='\\t')\n",
        "prev = ''\n",
        "tknzr = RegexpTokenizer(r'\\w+')\n",
        "ListOfCleanTokens = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E8lrxKKXM-QS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This method has been explained in [POS ngrams](https://github.com/OmarMeriwani/CE807-Sentiment-analysis/blob/master/POS_Ngrams.ipynb), it is a mistake to re-write it here :)"
      ]
    },
    {
      "metadata": {
        "id": "G45M32VzQivX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def getNgram(tags,gram):\n",
        "    counter = 0\n",
        "    ngrams = []\n",
        "    while counter < len(tags):\n",
        "        if counter + gram <= len(tags) - 1:\n",
        "            temp = []\n",
        "            for i in range(counter, counter + gram):\n",
        "                temp.append(tags[i])\n",
        "            ngrams.append(''.join(temp))\n",
        "            counter += 1\n",
        "        else:\n",
        "            break\n",
        "    return ngrams\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EQrauESWQi8W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The data set that would be resulted after the loop below should contain the following features."
      ]
    },
    {
      "metadata": {
        "id": "S3giGXUiQjLn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mode = 's'\n",
        "results = pd.DataFrame(columns=['phraseID','sentenceID','BigramsPolarity','UnigramsPolarity','SenticnetAVG','senticnetMAX','WordsInScore','POSSequenceScore','y'])\n",
        "j = 0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jjoPJ8QHQjSf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "j1fkrjSKDWnU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(0,len(df)):\n",
        "  '''\n",
        "  Similar to other document, this step specifies either to read only full sentences or all the sentences in the training dataset\n",
        "  '''\n",
        "    if mode != 'all':\n",
        "        if prev != str(df.loc[i][1]):\n",
        "            sentence = df.loc[i][2]\n",
        "            prev = str(df.loc[i][1])\n",
        "        else:\n",
        "            continue\n",
        "    else:\n",
        "        sentence = df.loc[i][2]\n",
        "\n",
        "    '''\n",
        "    Tokenize, get values from training dataset, get NER, and POS tags\n",
        "    '''\n",
        "    sentences = sent_tokenize(sentence)\n",
        "    reviewPolarity = int(df.loc[i][3])\n",
        "    POSSEQPolarity = 0\n",
        "    phraseID = df.loc[i][0]\n",
        "    sentenceID = df.loc[i][1]\n",
        "\n",
        "    tokens = []\n",
        "    for sent in sentences:\n",
        "        t = tknzr.tokenize(sent)\n",
        "        for tk in t:\n",
        "            tokens.append(tk)\n",
        "    NER = scnlp.ner(sentence)\n",
        "    POStagged = scnlp.pos_tag(sentence)\n",
        "    POSTags = [p for word, p in POStagged]\n",
        "\n",
        "    '''\n",
        "    Get the existing POS ngrams in each sentence and insure that no punctuation inside the sets.\n",
        "    '''\n",
        "    POSTriGrams = getNgram(POSTags,3)\n",
        "    POSQuadriGrams = getNgram(POSTags,4)\n",
        "    POSPentaGrams = getNgram(POSTags,5)\n",
        "    POSTriGrams = [p.replace(',','').replace(':','') for p in POSTriGrams]\n",
        "    POSPentaGrams = [p.replace(',','').replace(':','') for p in POSPentaGrams]\n",
        "    POSQuadriGrams = [p.replace(',','').replace(':','') for p in POSQuadriGrams]\n",
        "\n",
        "    \n",
        "    '''\n",
        "    First feature: the polarity of POS sequences equals the number of occurences of POS ngram multiplied by it's polarity multiplied by the ngram range (3,4,5)\n",
        "    '''\n",
        "    for possequence, count, pospolarity in tgrams:\n",
        "        if possequence in POSTriGrams:\n",
        "            POSSEQPolarity += count * pospolarity * 3\n",
        "\n",
        "    for possequence, count, pospolarity in qgrams:\n",
        "        if possequence in POSQuadriGrams:\n",
        "            POSSEQPolarity += count * pospolarity * 4\n",
        "\n",
        "    for possequence, count, pospolarity in pgrams:\n",
        "        if possequence in POSPentaGrams:\n",
        "            POSSEQPolarity += count * pospolarity * 5\n",
        "    \n",
        "    '''\n",
        "    Second and third features, the occurence of negative or positive unigrams or bigrams\n",
        "    '''\n",
        "    sentenceClean = ' '.join([str(t).lower() for t in tokens if t not in stop_words])\n",
        "    polarity2 = 0\n",
        "    polarity3 = 0\n",
        "    for pb in pbigrams:\n",
        "        pbigram = pb[1] + ' ' + pb[2]\n",
        "        if pbigram in sentenceClean:\n",
        "            if pb[3] == 4:\n",
        "                polarity2 += 1\n",
        "            else:\n",
        "                polarity2 -= 1\n",
        "    polarity1 = 0\n",
        "    for up in punigrams:\n",
        "        punigram = str(up[1])\n",
        "        if punigram in sentenceClean:\n",
        "            if up[2] == 4:\n",
        "                polarity1 += 1\n",
        "            else:\n",
        "                polarity1 -= 1\n",
        "    \n",
        "    '''\n",
        "    Fourth feature, if positive or negative words exist, words that were extracted in a different way than the previous two features\n",
        "    '''\n",
        "    for pw in positiveWords:\n",
        "        if pw[0] in sentenceClean and pw[1] >=5 :\n",
        "            polarity3 += 1\n",
        "    for nw in negativeWords:\n",
        "        if nw[0] in sentenceClean and nw[1] >=5 :\n",
        "            polarity3 -= 1\n",
        "    \n",
        "    cleanTokens = [str(t).lower() for t in tokens if t not in stop_words]\n",
        "    '''\n",
        "    Get max and avg polarity from SenticNet tool\n",
        "    '''\n",
        "    avgAndmaxPol = SentimentsPolarity(cleanTokens)\n",
        "    '''\n",
        "    Store the results.\n",
        "    '''\n",
        "    r = [ phraseID, sentenceID, polarity2, polarity1,avgAndmaxPol[0],avgAndmaxPol[1], polarity3, POSSEQPolarity, reviewPolarity]\n",
        "    results.loc[j] = r\n",
        "    print(r)\n",
        "    j += 1\n",
        "results.to_csv('TrainingDataset.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}