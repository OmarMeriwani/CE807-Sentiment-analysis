{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Frequent Subjective Words.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarMeriwani/CE807-Sentiment-analysis/blob/master/Frequent_Subjective_Words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7_Xs1s5VJbAM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This file contains the code of one of the features that has been used in features extraction file. The method is based on selecting words with specific POS tags, and storing them with the value assigned to them in training file. In addition to the number of occurances in the whole corpus. The NER words were removed from the final sets.   "
      ]
    },
    {
      "metadata": {
        "id": "M6QuK5TeKT_U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from stanfordcorenlp import StanfordCoreNLP\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import os\n",
        "from senticnet.senticnet import SenticNet\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "java_path = \"C:/Program Files/Java/jdk1.8.0_161/bin/java.exe\"\n",
        "os.environ['JAVAHOME'] = java_path\n",
        "host='http://localhost'\n",
        "port=9000\n",
        "scnlp =StanfordCoreNLP(host, port=port,lang='en', timeout=30000)\n",
        "stemmer = PorterStemmer()\n",
        "stop_words = stopwords.words('english')\n",
        "df = pd.read_csv('train.csv',header=0,sep='\\t')\n",
        "prev = ''\n",
        "tknzr = RegexpTokenizer(r'\\w+')\n",
        "ListOfCleanTokens = []\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zlYK4BRSKUKK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Only full sentences were used in this feature extraction step, in the loop below, the steps are going to be done on word level, inside each sentence."
      ]
    },
    {
      "metadata": {
        "id": "wBKfg34CKUTu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(0,len(df)):\n",
        "    if prev != str(df.loc[i][1]):\n",
        "        sentence = df.loc[i][2]\n",
        "        prev = str(df.loc[i][1])\n",
        "    else:\n",
        "        continue\n",
        "    #^^ Above lines are to choose only full sentences from the training set.\n",
        "    \n",
        "    '''\n",
        "    Tokenize, Get NER and POS tags\n",
        "    '''\n",
        "    sentences = sent_tokenize(sentence)\n",
        "    reviewPolarity = int(df.loc[i][3])\n",
        "    tokens = []\n",
        "    for sent in sentences:\n",
        "        t = tknzr.tokenize(sent)\n",
        "        for tk in t:\n",
        "            tokens.append(tk)\n",
        "    NER = scnlp.ner(sentence)\n",
        "    POStagged = scnlp.pos_tag(sentence)\n",
        "    \n",
        "    '''\n",
        "    Remove stop words, convert to lower case and choose only unique set of words from the sentence\n",
        "    '''\n",
        "    cleanTokensWithoutNegation = [str(t).lower() for t in tokens if t not in stop_words]\n",
        "    cleanTokensWithoutNegation = set(cleanTokensWithoutNegation)\n",
        "    '''\n",
        "    Remove NER and choose specific POS Tags, then only words that are not NER and within the set of tags will be chosen and added to the dataset.\n",
        "    '''\n",
        "    usefultags = ['JJ','JJR','JJS','RB ','VB ','VBD','VBG','VBN','VBP','VBZ']\n",
        "    cleanTokensWithoutNegation = [t for t in cleanTokensWithoutNegation\n",
        "                                  if t not in\n",
        "                                  [str(n).lower() for n, nrt in NER if str(nrt).lower() != 'o'] and\n",
        "                                  t in [str(word).lower() for word, p in POStagged if p in usefultags ]]\n",
        "    ListOfCleanTokens.append([cleanTokensWithoutNegation, reviewPolarity])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ARJNrIiMKUa-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After preparing the set of words with their polarity, and that are within the POS tag set and are not NER, the count of each word will be set, and according to the polarity, the set is going to be divided into positive words (the words that comes only within the positive reviews) and negative words."
      ]
    },
    {
      "metadata": {
        "id": "qAwPP295MEwV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "GoodWords = {}\n",
        "BadWords = {}\n",
        "for i in ListOfCleanTokens:\n",
        "    cleanTokens = i[0]\n",
        "    reviewPolarity = i[1]\n",
        "    if reviewPolarity > 2:\n",
        "        for t in cleanTokens:\n",
        "            if t not in GoodWords:\n",
        "                GoodWords[t] = 1\n",
        "            else:\n",
        "                GoodWords[t] = GoodWords.get(t) + 1\n",
        "    if reviewPolarity < 2:\n",
        "        for t in cleanTokens:\n",
        "            if t not in BadWords:\n",
        "                BadWords[t] = 1\n",
        "            else:\n",
        "                BadWords[t] = BadWords.get(t) + 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t6zwRzoEME7C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Remove the common words between the two lists. and store the lists."
      ]
    },
    {
      "metadata": {
        "id": "sN25DZLkJTi5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "intersection = [t for t,i in GoodWords.items() if t in BadWords]\n",
        "for i in intersection:\n",
        "    if i in GoodWords:\n",
        "        GoodWords.pop(i)\n",
        "for i in intersection:\n",
        "    if i in BadWords:\n",
        "        BadWords.pop(i)\n",
        "\n",
        "print(GoodWords)\n",
        "print(BadWords)\n",
        "import csv\n",
        "with open('PositiveWords.csv', 'w') as f:\n",
        "    for key in GoodWords.keys():\n",
        "        f.write(\"%s,%s\\n\"%(key,GoodWords[key]))\n",
        "with open('NegativeWords.csv', 'w') as f:\n",
        "    for key in BadWords.keys():\n",
        "        f.write(\"%s,%s\\n\"%(key,BadWords[key]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}